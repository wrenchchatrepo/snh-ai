{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNH-AI Customer Segmentation Analysis\n",
    "\n",
    "This notebook analyzes the customer segments identified by the KMeans clustering model and explores predictions for transaction counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from supabase import create_client, Client\n",
    "import joblib # To load saved models/preprocessors\n",
    "import subprocess # To optionally run the setup and main pipeline\n",
    "\n",
    "# Add project root to path to import config and logger\n",
    "project_root = os.path.abspath('.') # Assumes notebook is run from dev/snh-ai\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "if src_path not in sys.path:\n",
    "     sys.path.insert(0, src_path)\n",
    "\n",
    "try:\n",
    "    import config\n",
    "    from src import snh_logger as snh_logging\n",
    "    logger = snh_logging.get_logger(\"AnalysisNotebook\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}. Ensure config.py and src/snh_logger.py exist.\")\n",
    "    # Fallback basic logger if needed\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"AnalysisNotebook_Fallback\")\n",
    "    logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. (Optional) Run Setup and Full Data Pipeline\n",
    "\n",
    "Uncomment and modify the cell below (`run_setup = True`, `run_pipeline = True`) **only** if you need to perform initial setup (directory checks, `.env.example` creation via `setup.py`) AND regenerate all the data in the Supabase tables (`raw_customer_data`, `cleaned_customer_data`, `transformed_customer_data`, `customer_segments`) from scratch by executing the `process.py` script. \n",
    "\n",
    "**Warning:** Running `process.py` will clear existing data in those tables and replace it based on the current pipeline configuration. It may take several seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set flags to control execution\n",
    "run_setup = False # Set to True to run setup.py first\n",
    "run_pipeline = False # Set to True to run the full process.py pipeline\n",
    "\n",
    "def run_script(script_name):\n",
    "    # Use the python executable from the currently running kernel/env\n",
    "    python_executable = sys.executable \n",
    "    command = [python_executable, script_name]\n",
    "    try:\n",
    "        print(f\"Running script: {' '.join(command)}...\")\n",
    "        # Use check=True to raise an error if the script fails\n",
    "        # Capture output to display it in the notebook\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True, cwd=project_root)\n",
    "        print(f\"--- {script_name} stdout: ---\")\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "             print(f\"--- {script_name} stderr: ---\")\n",
    "             print(result.stderr)\n",
    "        print(f\"--- {script_name} execution complete. ---\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: '{python_executable}' command not found or script '{script_name}' not found. Make sure Python is in your PATH and the script exists.\")\n",
    "        return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ERROR: {script_name} failed with exit code {e.returncode}\")\n",
    "        print(f\"--- {script_name} stdout (error): ---\")\n",
    "        print(e.stdout)\n",
    "        print(f\"--- {script_name} stderr (error): ---\")\n",
    "        print(e.stderr)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while running {script_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Execution Control --- \n",
    "setup_success = True # Assume success if setup is skipped\n",
    "if run_setup:\n",
    "    print(\"*** Executing setup.py ***\")\n",
    "    setup_success = run_script('setup.py')\n",
    "    if not setup_success:\n",
    "         print(\"Halting due to setup.py failure.\")\n",
    "    else:\n",
    "         print(\"*** Setup script finished. ***\")\n",
    "else:\n",
    "    print(\"Setup execution skipped (run_setup=False).\")\n",
    "    \n",
    "# Run pipeline only if setup was skipped or succeeded\n",
    "pipeline_run_needed = run_pipeline and setup_success\n",
    "\n",
    "if pipeline_run_needed:\n",
    "     print(\"\\n*** Executing process.py (Full Pipeline) ***\")\n",
    "     pipeline_success = run_script('process.py')\n",
    "     if not pipeline_success:\n",
    "         print(\"Pipeline execution failed.\")\n",
    "     else:\n",
    "         print(\"*** Full pipeline execution finished. ***\")\n",
    "elif run_pipeline and not setup_success:\n",
    "    # Case where setup failed and pipeline was requested\n",
    "    print(\"Skipped pipeline execution because setup failed.\")\n",
    "else:\n",
    "    print(\"\\nPipeline execution skipped (run_pipeline=False).\")\n",
    "\n",
    "# Default message if both are false\n",
    "if not run_setup and not run_pipeline:\n",
    "     print(\"Setup and Pipeline execution skipped (set run_setup=True and/or run_pipeline=True in cell to execute).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Supabase Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials and initialize Supabase client\n",
    "supabase: Client | None = None\n",
    "try:\n",
    "    if not config.SUPABASE_URL or not config.SUPABASE_SERVICE_ROLE:\n",
    "        logger.error(\"Supabase URL or Service Role not found in config.\")\n",
    "    else:\n",
    "        supabase = create_client(config.SUPABASE_URL, config.SUPABASE_SERVICE_ROLE)\n",
    "        logger.info(\"Supabase client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Supabase client: {e}\")\n",
    "\n",
    "# Table names\n",
    "CLEANED_TABLE = 'cleaned_customer_data'\n",
    "SEGMENTS_TABLE = 'customer_segments'\n",
    "TRANSFORMED_TABLE = 'transformed_customer_data' # Optional for plotting scaled data\n",
    "PREDICTIONS_TABLE = 'transaction_predictions' # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(supabase_client: Client, table_name: str, select_cols: str = \"*\") -> pd.DataFrame:\n",
    "    \"\"\"Fetches data from a Supabase table into a pandas DataFrame.\"\"\"\n",
    "    df = pd.DataFrame() # Initialize empty DataFrame\n",
    "    if not supabase_client:\n",
    "        logger.error(\"Supabase client is not initialized.\")\n",
    "        return df\n",
    "    try:\n",
    "        logger.info(f\"Fetching data from {table_name}... ({select_cols})\")\n",
    "        response = supabase_client.table(table_name).select(select_cols).execute()\n",
    "        if response.data:\n",
    "            df = pd.DataFrame(response.data)\n",
    "            logger.info(f\"Successfully fetched {len(df)} rows from {table_name}.\")\n",
    "        else:\n",
    "            logger.warning(f\"No data returned from {table_name}. Check if pipeline ran correctly.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data from {table_name}: {e}\")\n",
    "    return df\n",
    "\n",
    "# Fetch cleaned data (contains original features)\n",
    "cleaned_df = fetch_data(supabase, CLEANED_TABLE)\n",
    "# Ensure correct types from DB fetch (Supabase client sometimes returns things as strings/objects)\n",
    "if not cleaned_df.empty:\n",
    "    cleaned_df['age'] = pd.to_numeric(cleaned_df['age'], errors='coerce').astype('Int64')\n",
    "    cleaned_df['annual_income'] = pd.to_numeric(cleaned_df['annual_income'], errors='coerce')\n",
    "    cleaned_df['total_transactions'] = pd.to_numeric(cleaned_df['total_transactions'], errors='coerce').astype('Int64')\n",
    "    cleaned_df['customer_id'] = cleaned_df['customer_id'].astype(str)\n",
    "    cleaned_df['region'] = cleaned_df['region'].astype(str)\n",
    "\n",
    "# Fetch segment assignments\n",
    "segments_df = fetch_data(supabase, SEGMENTS_TABLE, \"customer_id, pattern_id\")\n",
    "if not segments_df.empty:\n",
    "     segments_df['customer_id'] = segments_df['customer_id'].astype(str)\n",
    "     segments_df['pattern_id'] = pd.to_numeric(segments_df['pattern_id'], errors='coerce').astype('Int64') \n",
    "\n",
    "# Optional: Fetch transformed data for plotting scaled features\n",
    "transformed_df = fetch_data(supabase, TRANSFORMED_TABLE)\n",
    "if not transformed_df.empty:\n",
    "     transformed_df['customer_id'] = transformed_df['customer_id'].astype(str)\n",
    "     # Convert scaled/OHE columns back to numeric if needed\n",
    "     for col in transformed_df.columns:\n",
    "         if col != 'customer_id' and col != 'transformed_at':\n",
    "              transformed_df[col] = pd.to_numeric(transformed_df[col], errors='coerce')\n",
    "\n",
    "# Optional: Fetch predictions\n",
    "predictions_df = fetch_data(supabase, PREDICTIONS_TABLE)\n",
    "if not predictions_df.empty:\n",
    "    predictions_df['customer_id'] = predictions_df['customer_id'].astype(str)\n",
    "    predictions_df['predicted_total_transactions'] = pd.to_numeric(predictions_df['predicted_total_transactions'], errors='coerce')\n",
    "\n",
    "print(\"--- Cleaned Data Head: ---\")\n",
    "display(cleaned_df.head()) if not cleaned_df.empty else print(\"Cleaned data is empty.\")\n",
    "print(\"\\n--- Segment Assignments Head: ---\")\n",
    "display(segments_df.head()) if not segments_df.empty else print(\"Segment assignments data is empty.\")\n",
    "print(\"\\n--- Transformed Data Head (Optional): ---\")\n",
    "display(transformed_df.head()) if not transformed_df.empty else print(\"Transformed data is empty or not loaded.\")\n",
    "print(\"\\n--- Predictions Data Head (Optional): ---\")\n",
    "display(predictions_df.head()) if not predictions_df.empty else print(\"Predictions data is empty or not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge segment assignments with cleaned data\n",
    "merged_df = pd.DataFrame() # Initialize as empty\n",
    "if not cleaned_df.empty and not segments_df.empty:\n",
    "    merged_df = pd.merge(cleaned_df, segments_df, on='customer_id', how='left')\n",
    "    # Ensure pattern_id is treated as a categorical variable for plotting/analysis\n",
    "    if 'pattern_id' in merged_df.columns and not merged_df['pattern_id'].isnull().all():\n",
    "        merged_df['pattern_id'] = pd.Categorical(merged_df['pattern_id'])\n",
    "        logger.info(f\"Merged data shape: {merged_df.shape}\")\n",
    "        print(\"--- Merged Data Head: ---\")\n",
    "        display(merged_df.head())\n",
    "        print(\"\\n--- Merged Data Info: ---\")\n",
    "        merged_df.info()\n",
    "    else:\n",
    "        logger.warning(\"Merge completed, but 'pattern_id' column is missing or all null.\")\n",
    "else:\n",
    "    logger.error(\"Could not merge dataframes, cleaned_df or segments_df is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Segment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Segment Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not merged_df.empty and 'pattern_id' in merged_df.columns and merged_df['pattern_id'].notna().any():\n",
    "    segment_counts = merged_df['pattern_id'].value_counts().sort_index()\n",
    "    print(\"--- Segment Sizes (Value Counts): ---\")\n",
    "    display(segment_counts)\n",
    "\n",
    "    # Plotting segment sizes\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(data=merged_df.dropna(subset=['pattern_id']), x='pattern_id', palette='viridis', order=segment_counts.index)\n",
    "    plt.title('Customer Count per Segment (pattern_id)')\n",
    "    plt.xlabel('Segment (pattern_id)')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.warning(\"Merged DataFrame is empty or missing 'pattern_id'. Cannot analyze segment sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Feature Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for numerical features grouped by segment\n",
    "numerical_features = ['age', 'annual_income', 'total_transactions']\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns and merged_df['pattern_id'].notna().any():\n",
    "    # Ensure correct numeric types before aggregation\n",
    "    valid_numerical_features = []\n",
    "    for col in numerical_features:\n",
    "         if col in merged_df.columns:\n",
    "              merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "              valid_numerical_features.append(col)\n",
    "         else:\n",
    "              logger.warning(f\"Numerical feature '{col}' not found in merged_df for summary stats.\")\n",
    "\n",
    "    # Perform aggregation only if pattern_id is not all NaN and we have valid features\n",
    "    if merged_df['pattern_id'].notna().any() and valid_numerical_features:\n",
    "        segment_summary = merged_df.groupby('pattern_id', observed=False)[valid_numerical_features].agg(['mean', 'median', 'std', 'count'])\n",
    "        print(\"--- Summary Statistics by Segment: ---\")\n",
    "        # Display with rounded values for readability\n",
    "        try:\n",
    "            display(segment_summary.style.format(\"{:.2f}\")) # Use display() in Jupyter\n",
    "        except NameError:\n",
    "            print(segment_summary.round(2))\n",
    "    else:\n",
    "        logger.warning(\"'pattern_id' column contains only null values or no valid numerical features found. Cannot group for summary stats.\")\n",
    "else:\n",
    "    logger.warning(\"Cannot calculate segment summary statistics - check merged_df and pattern_id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features across segments\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns and merged_df['pattern_id'].notna().any():\n",
    "    for col in numerical_features: # Use the original list for iteration\n",
    "        if col in merged_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            # Ensure plot orders by category index if pattern_id is categorical\n",
    "            plot_order = sorted(merged_df['pattern_id'].cat.categories) if pd.api.types.is_categorical_dtype(merged_df['pattern_id']) else None\n",
    "            sns.boxplot(data=merged_df, x='pattern_id', y=col, palette='viridis', order=plot_order)\n",
    "            plt.title(f'Distribution of {col} by Segment (pattern_id)')\n",
    "            plt.xlabel('Segment (pattern_id)')\n",
    "            plt.ylabel(col)\n",
    "            plt.show()\n",
    "        else:\n",
    "             logger.warning(f\"Column '{col}' not found for plotting distribution.\")\n",
    "else:\n",
    "     logger.warning(\"Cannot visualize numerical feature distributions - check merged_df and pattern_id.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Region Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze region distribution within each segment\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns and 'region' in merged_df.columns and merged_df['pattern_id'].notna().any():\n",
    "    region_distribution = pd.crosstab(merged_df['pattern_id'], merged_df['region'])\n",
    "    region_distribution_norm = pd.crosstab(merged_df['pattern_id'], merged_df['region'], normalize='index') * 100\n",
    "    print(\"--- Region Distribution (Counts) within each Segment: ---\")\n",
    "    try:\n",
    "        display(region_distribution)\n",
    "    except NameError:\n",
    "        print(region_distribution)\n",
    "    print(\"\\n--- Region Distribution (%) within each Segment: ---\")\n",
    "    try:\n",
    "        display(region_distribution_norm.style.format(\"{:.1f}%\"))\n",
    "    except NameError:\n",
    "        print(region_distribution_norm.round(1).astype(str) + '%')\n",
    "\n",
    "    # Plotting the distribution\n",
    "    region_distribution_norm.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='viridis')\n",
    "    plt.title('Region Distribution by Customer Segment')\n",
    "    plt.xlabel('Segment (pattern_id)')\n",
    "    plt.ylabel('Percentage of Customers (%)')\n",
    "    plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "     logger.warning(\"Cannot analyze region distribution - check merged_df, pattern_id, region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization (Optional Advanced Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scatter plot using scaled features (requires transformed_df to be loaded)\n",
    "if not transformed_df.empty and not segments_df.empty:\n",
    "    plot_df = pd.merge(transformed_df, segments_df, on='customer_id', how='left')\n",
    "    if 'pattern_id' in plot_df.columns and not plot_df['pattern_id'].isnull().all():\n",
    "        plot_df['pattern_id'] = pd.Categorical(plot_df['pattern_id'])\n",
    "        \n",
    "        # Check if required scaled columns exist\n",
    "        if 'age_scaled' in plot_df.columns and 'annual_income_scaled' in plot_df.columns:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            sns.scatterplot(data=plot_df.dropna(subset=['pattern_id']), \\\n",
    "                            x='age_scaled', y='annual_income_scaled', \\\n",
    "                            hue='pattern_id', palette='viridis', s=70, alpha=0.7)\n",
    "            plt.title('Customer Segments based on Scaled Age and Income')\n",
    "            plt.xlabel('Age (Standardized)')\n",
    "            plt.ylabel('Annual Income (Standardized)')\n",
    "            plt.legend(title='Segment (pattern_id)')\n",
    "            plt.show()\n",
    "        else:\n",
    "            logger.warning(\"Scaled columns ('age_scaled', 'annual_income_scaled') not found. Skipping scatter plot.\")\n",
    "    else:\n",
    "         logger.warning(\"Segment IDs missing or all null after merge, skipping scatter plot.\")\n",
    "else:\n",
    "     logger.warning(\"Transformed data or segment data not loaded or empty, skipping scatter plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Model Insights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved RandomForestRegressor model and preprocessor for transaction prediction\n",
    "model_dir = config.MODEL_OUTPUT_DIR if hasattr(config, 'MODEL_OUTPUT_DIR') else 'models'\n",
    "model_filename = os.path.join(model_dir, \"transactions_predictor_model.joblib\")\n",
    "preprocessor_filename = os.path.join(model_dir, \"transactions_predictor_preprocessor.joblib\")\n",
    "\n",
    "try:\n",
    "    rf_model = joblib.load(model_filename)\n",
    "    preprocessor = joblib.load(preprocessor_filename)\n",
    "    logger.info(\"Successfully loaded transactions predictor model and preprocessor.\")\n",
    "\n",
    "    # Get feature importances\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        # Get feature names from the preprocessor\n",
    "        try:\n",
    "             ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(['region'])\n",
    "             # Extract passthrough feature names correctly\n",
    "             if hasattr(preprocessor, 'feature_names_in_'):\n",
    "                 input_features = preprocessor.feature_names_in_\n",
    "                 original_cat_cols = preprocessor.transformers_[0][2] # Assuming OHE is first\n",
    "                 passthrough_features = [col for col in input_features if col not in original_cat_cols]\n",
    "                 processed_feature_names = np.concatenate([ohe_feature_names, passthrough_features])\n",
    "             else:\n",
    "                  passthrough_features = ['age'] # Fallback assumption\n",
    "                  processed_feature_names = np.concatenate([ohe_feature_names, passthrough_features])\n",
    "\n",
    "        except Exception as e_feat:\n",
    "             logger.warning(f\"Could not get feature names from preprocessor: {e_feat}\")\n",
    "             # Fallback: generate generic feature names based on number of importances\n",
    "             num_features = len(rf_model.feature_importances_)\n",
    "             processed_feature_names = [f'feature_{i}' for i in range(num_features)]\n",
    "\n",
    "        importances = rf_model.feature_importances_\n",
    "\n",
    "        # Ensure lengths match\n",
    "        if len(processed_feature_names) == len(importances):\n",
    "            feature_importance_df = pd.DataFrame({'feature': processed_feature_names, 'importance': importances})\n",
    "            feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "            print(\"--- Feature Importances for Predicting Total Transactions: ---\")\n",
    "            try:\n",
    "                display(feature_importance_df)\n",
    "            except NameError:\n",
    "                 print(feature_importance_df)\n",
    "\n",
    "            # Plot feature importances\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')\n",
    "            plt.title('Feature Importance for Predicting Total Transactions')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            logger.warning(f\"Mismatch between number of feature names ({len(processed_feature_names)}) and importances ({len(importances)}). Skipping importance plot.\")\n",
    "\n",
    "    else:\n",
    "        logger.warning(\"Could not retrieve feature importances from the loaded model.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Model or preprocessor file not found. Ensure '{model_filename}' and '{preprocessor_filename}' exist.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading or analyzing predictive model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Insights & Conclusion\n",
    "\n",
    "*   **(Fill In)** Summary of Segments: Briefly describe the key characteristics of each identified segment (pattern_id 0-5) based on the analysis above (e.g., \\\"Segment 0 represents younger customers with lower income and fewer transactions, primarily from region X\\\").\n",
    "*   **(Fill In)** Predictive Insights: Mention key findings from the RandomForestRegressor if analyzed (e.g., \\\"Age was found to be the most important predictor of total transactions... Region X also showed higher transaction counts...\\\").\n",
    "*   **(Fill In)** Business Implications: Suggest potential business actions based on these segments (e.g., targeted marketing campaigns, different service levels, promotions for specific regions/age groups).\n",
    "*   **(Fill In)** Future Work: Mention potential next steps (e.g., using different clustering algorithms, adding more features, deploying models, A/B testing strategies based on segments)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
