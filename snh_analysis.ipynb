```json
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNH-AI Customer Segmentation Analysis\n",
    "\n",
    "This notebook analyzes the customer segments identified by the KMeans clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from supabase import create_client, Client\n",
    "import joblib # To load saved models/preprocessors\n",
    "\n",
    "# Add project root to path to import config and logger\n",
    "project_root = os.path.abspath('.') # Assumes notebook is run from dev/snh-ai\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "if os.path.join(project_root, 'src') not in sys.path:\n",
    "     sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "try:\n",
    "    import config\n",
    "    from src import snh_logger as snh_logging\n",
    "    logger = snh_logging.get_logger(\"AnalysisNotebook\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}. Ensure config.py and src/snh_logger.py exist.\")\n",
    "    # Fallback basic logger if needed\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"AnalysisNotebook_Fallback\")\n",
    "    logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Supabase Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials and initialize Supabase client\n",
    "supabase: Client | None = None\n",
    "try:\n",
    "    if not config.SUPABASE_URL or not config.SUPABASE_SERVICE_ROLE:\n",
    "        logger.error(\"Supabase URL or Service Role not found in config.\")\n",
    "    else:\n",
    "        supabase = create_client(config.SUPABASE_URL, config.SUPABASE_SERVICE_ROLE)\n",
    "        logger.info(\"Supabase client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Supabase client: {e}\")\n",
    "\n",
    "# Table names\n",
    "CLEANED_TABLE = 'cleaned_customer_data'\n",
    "SEGMENTS_TABLE = 'customer_segments'\n",
    "TRANSFORMED_TABLE = 'transformed_customer_data' # Optional for plotting scaled data\n",
    "PREDICTIONS_TABLE = 'transaction_predictions' # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(supabase_client: Client, table_name: str, select_cols: str = \"*\") -> pd.DataFrame:\n",
    "    \"\"\"Fetches data from a Supabase table into a pandas DataFrame.\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    if not supabase_client:\n",
    "        logger.error(\"Supabase client is not initialized.\")\n",
    "        return df\n",
    "    try:\n",
    "        logger.info(f\"Fetching data from {table_name}...\")\n",
    "        response = supabase_client.table(table_name).select(select_cols).execute()\n",
    "        if response.data:\n",
    "            df = pd.DataFrame(response.data)\n",
    "            logger.info(f\"Successfully fetched {len(df)} rows from {table_name}.\")\n",
    "        else:\n",
    "            logger.warning(f\"No data returned from {table_name}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data from {table_name}: {e}\")\n",
    "    return df\n",
    "\n",
    "# Fetch cleaned data (contains original features)\n",
    "cleaned_df = fetch_data(supabase, CLEANED_TABLE)\n",
    "# Ensure correct types from DB fetch (Supabase client sometimes returns things as strings/objects)\n",
    "if not cleaned_df.empty:\n",
    "    cleaned_df['age'] = pd.to_numeric(cleaned_df['age'], errors='coerce').astype('Int64')\n",
    "    cleaned_df['annual_income'] = pd.to_numeric(cleaned_df['annual_income'], errors='coerce')\n",
    "    cleaned_df['total_transactions'] = pd.to_numeric(cleaned_df['total_transactions'], errors='coerce').astype('Int64')\n",
    "    cleaned_df['customer_id'] = cleaned_df['customer_id'].astype(str)\n",
    "    cleaned_df['region'] = cleaned_df['region'].astype(str)\n",
    "    \n",
    "# Fetch segment assignments\n",
    "segments_df = fetch_data(supabase, SEGMENTS_TABLE, \"customer_id, pattern_id\")\n",
    "if not segments_df.empty:\n",
    "     segments_df['customer_id'] = segments_df['customer_id'].astype(str)\n",
    "     segments_df['pattern_id'] = pd.to_numeric(segments_df['pattern_id'], errors='coerce').astype('Int64') # Or just int if no NAs\n",
    "\n",
    "# Optional: Fetch transformed data for plotting scaled features\n",
    "# transformed_df = fetch_data(supabase, TRANSFORMED_TABLE)\n",
    "\n",
    "# Optional: Fetch predictions\n",
    "# predictions_df = fetch_data(supabase, PREDICTIONS_TABLE)\n",
    "\n",
    "print(\"Cleaned Data Head:\")\n",
    "print(cleaned_df.head())\n",
    "print(\"\\nSegment Assignments Head:\")\n",
    "print(segments_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge segment assignments with cleaned data\n",
    "if not cleaned_df.empty and not segments_df.empty:\n",
    "    merged_df = pd.merge(cleaned_df, segments_df, on='customer_id', how='left')\n",
    "    # Ensure pattern_id is treated as a categorical variable for plotting/analysis\n",
    "    if 'pattern_id' in merged_df.columns:\n",
    "        merged_df['pattern_id'] = pd.Categorical(merged_df['pattern_id'])\n",
    "    logger.info(f\"Merged data shape: {merged_df.shape}\")\n",
    "    print(\"\\nMerged Data Head:\")\n",
    "    print(merged_df.head())\n",
    "    print(\"\\nMerged Data Info:\")\n",
    "    merged_df.info()\n",
    "else:\n",
    "    logger.error(\"Could not merge dataframes, one or both are empty.\")\n",
    "    merged_df = pd.DataFrame() # Assign empty df to avoid errors later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Segment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Segment Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not merged_df.empty and 'pattern_id' in merged_df.columns:\n",
    "    segment_counts = merged_df['pattern_id'].value_counts().sort_index()\n",
    "    print(\"Segment Sizes (Value Counts):\")\n",
    "    print(segment_counts)\n",
    "    \n",
    "    # Plotting segment sizes\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(data=merged_df, x='pattern_id', palette='viridis', order=segment_counts.index)\n",
    "    plt.title('Customer Count per Segment (pattern_id)')\n",
    "    plt.xlabel('Segment (pattern_id)')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.warning(\"Merged DataFrame is empty or missing 'pattern_id'. Cannot analyze segment sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Feature Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for numerical features grouped by segment\n",
    "numerical_features = ['age', 'annual_income', 'total_transactions']\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns:\n",
    "    # Ensure correct numeric types before aggregation\n",
    "    for col in numerical_features:\n",
    "         merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "    segment_summary = merged_df.groupby('pattern_id', observed=False)[numerical_features].agg(['mean', 'median', 'std', 'count'])\n",
    "    print(\"\\nSummary Statistics by Segment:\")\n",
    "    # Display with rounded values for readability\n",
    "    try:\n",
    "        display(segment_summary.style.format(\"{:.2f}\")) # Use display() in Jupyter\n",
    "    except NameError:\n",
    "        print(segment_summary.round(2))\n",
    "else:\n",
    "    logger.warning(\"Cannot calculate segment summary statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of numerical features across segments\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns:\n",
    "    for col in numerical_features:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=merged_df, x='pattern_id', y=col, palette='viridis', order=sorted(merged_df['pattern_id'].cat.categories))\n",
    "        plt.title(f'Distribution of {col} by Segment (pattern_id)')\n",
    "        plt.xlabel('Segment (pattern_id)')\n",
    "        plt.ylabel(col)\n",
    "        plt.show()\n",
    "else:\n",
    "     logger.warning(\"Cannot visualize numerical feature distributions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Region Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze region distribution within each segment\n",
    "if not merged_df.empty and 'pattern_id' in merged_df.columns and 'region' in merged_df.columns:\n",
    "    region_distribution = pd.crosstab(merged_df['pattern_id'], merged_df['region'])\n",
    "    region_distribution_norm = pd.crosstab(merged_df['pattern_id'], merged_df['region'], normalize='index') * 100\n",
    "    print(\"\\nRegion Distribution (Counts) within each Segment:\")\n",
    "    try:\n",
    "        display(region_distribution)\n",
    "    except NameError:\n",
    "        print(region_distribution)\n",
    "    print(\"\\nRegion Distribution (%) within each Segment:\")\n",
    "    try:\n",
    "        display(region_distribution_norm.style.format(\"{:.1f}%\"))\n",
    "    except NameError:\n",
    "        print(region_distribution_norm.round(1).astype(str) + '%')\n",
    "    \n",
    "    # Plotting the distribution\n",
    "    region_distribution_norm.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='viridis')\n",
    "    plt.title('Region Distribution by Customer Segment')\n",
    "    plt.xlabel('Segment (pattern_id)')\n",
    "    plt.ylabel('Percentage of Customers (%)')\n",
    "    plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "     logger.warning(\"Cannot analyze region distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization (Optional Advanced Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scatter plot using scaled features (requires fetching transformed_df)\n",
    "# try:\n",
    "#     transformed_df = fetch_data(supabase, TRANSFORMED_TABLE)\n",
    "#     if not transformed_df.empty and 'pattern_id' in merged_df.columns:\n",
    "#         plot_df = pd.merge(transformed_df, segments_df, on='customer_id', how='left')\n",
    "#         if 'pattern_id' in plot_df.columns:\n",
    "#             plot_df['pattern_id'] = pd.Categorical(plot_df['pattern_id'])\n",
    "#             plt.figure(figsize=(10, 7))\n",
    "#             sns.scatterplot(data=plot_df, x='age_scaled', y='annual_income_scaled', hue='pattern_id', palette='viridis', s=70, alpha=0.7)\n",
    "#             plt.title('Customer Segments based on Scaled Age and Income')\n",
    "#             plt.xlabel('Age (Standardized)')\n",
    "#             plt.ylabel('Annual Income (Standardized)')\n",
    "#             plt.legend(title='Segment (pattern_id)')\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#              logger.warning(\"Segment IDs missing, skipping scatter plot.\")\n",
    "#     else:\n",
    "#          logger.warning(\"Transformed data not loaded or empty, skipping scatter plot.\")\n",
    "# except Exception as e_plot:\n",
    "#     logger.error(f\"Error generating scatter plot: {e_plot}\")\n",
    "pass # Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Model Insights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved RandomForestRegressor model and preprocessor for transaction prediction\n",
    "model_filename = os.path.join(config.MODEL_OUTPUT_DIR if hasattr(config, 'MODEL_OUTPUT_DIR') else 'models', \"transactions_predictor_model.joblib\")\n",
    "preprocessor_filename = os.path.join(config.MODEL_OUTPUT_DIR if hasattr(config, 'MODEL_OUTPUT_DIR') else 'models', \"transactions_predictor_preprocessor.joblib\")\n",
    "\n",
    "try:\n",
    "    rf_model = joblib.load(model_filename)\n",
    "    preprocessor = joblib.load(preprocessor_filename)\n",
    "    logger.info(\"Successfully loaded transactions predictor model and preprocessor.\")\n",
    "    \n",
    "    # Get feature importances\n",
    "    if hasattr(rf_model, 'feature_importances_'):\n",
    "        # Get feature names from the preprocessor\n",
    "        try:\n",
    "             ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(['region'])\n",
    "             passthrough_features = ['age'] # Hardcoded based on predictive_model.py\n",
    "             processed_feature_names = np.concatenate([ohe_feature_names, passthrough_features])\n",
    "        except Exception as e_feat:\n",
    "             logger.warning(f\"Could not get feature names from preprocessor: {e_feat}\")\n",
    "             processed_feature_names = [f'feature_{i}' for i in range(len(rf_model.feature_importances_))]\n",
    "             \n",
    "        importances = rf_model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'feature': processed_feature_names, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importances for Predicting Total Transactions:\")\n",
    "        try:\n",
    "            display(feature_importance_df)\n",
    "        except NameError:\n",
    "             print(feature_importance_df)\n",
    "        \n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=feature_importance_df, x='importance', y='feature', palette='viridis')\n",
    "        plt.title('Feature Importance for Predicting Total Transactions')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.warning(\"Could not retrieve feature importances from the loaded model.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Model or preprocessor file not found. Ensure '{model_filename}' and '{preprocessor_filename}' exist.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading or analyzing predictive model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Insights & Conclusion\n",
    "\n",
    "*   **Summary of Segments:** Briefly describe the key characteristics of each identified segment (pattern_id 0-5) based on the analysis above (e.g., \"Segment 0 represents younger customers with lower income and fewer transactions, primarily from region X\").\n",
    "*   **Predictive Insights:** Mention key findings from the RandomForestRegressor if analyzed (e.g., \"Age was found to be the most important predictor of total transactions...\").\n",
    "*   **Business Implications:** Suggest potential business actions based on these segments (e.g., targeted marketing campaigns, different service levels).\n",
    "*   **Future Work:** Mention potential next steps (e.g., using different clustering algorithms, adding more features, deploying models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
```