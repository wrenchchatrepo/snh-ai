# SNH-AI Project

This project implements an end-to-end data pipeline to extract customer data, transform it, and apply a machine learning model to identify customer segments.

## Overview

The pipeline consists of the following main stages:
1.  **Data _E_xtraction**: Retrieves customer data from a source.
2.  **Data _T_ransformation**: Cleans, scales, and encodes the data.
3.  **Machine Learning**: Applies a clustering model to identify patterns.
4.  **Data _L_oading**: Loads the processed data with pattern labels to a destination.

## Project Structure

```
snh-ai/
├── .env                # Environment variables (generated by setup.py, not committed)
├── .github/            # GitHub specific files (e.g., workflows)
├── .gitignore          # Specifies intentionally untracked files that Git should ignore
├── Dockerfile          # For containerizing the application
├── README.md           # This file
├── config.py           # Configuration file for the project
├── docs/               # Documentation files
│   ├── StandardScaler.md
│   └── One-hot_encode.md
├── mcp-servers/        # MCP server configurations/stubs
│   ├── bayes-mcp
│   ├── mcp-server-axiom
│   ├── mcp-server-context7
│   ├── mcp-server-github
│   ├── mcp-server-sequential-thinking
│   └── postgres-context-server
├── process.py          # Main orchestration script for the pipeline
├── requirements.txt    # Python dependencies (to be generated)
├── setup.py            # Script for project setup and initialization
├── src/                # Source code for the pipeline
│   ├── Ingest.py       # Data extraction script
│   ├── clean.py        # Data cleaning script
│   ├── encode.py       # Data encoding script
│   ├── logging.py      # Common logging module
│   ├── ml_model.py     # Machine learning model script
│   ├── presentation.py # Data loading/presentation script
│   ├── scale.py        # Data scaling script
│   └── template.json   # Template for script structure
└── start-database.sh   # Script to start/setup the database
```

## Getting Started

(Instructions to be added)

## Dependencies

(To be listed, likely in `requirements.txt`)

## Usage

(Instructions on how to run the pipeline to be added)

## ML Model Details

### Customer Segmentation (KMeans)

The pipeline uses KMeans clustering to identify customer segments based on the transformed data (`age_scaled`, `annual_income_scaled`, `total_transactions`, and one-hot encoded `region`).

#### Elbow Method for Optimal k

The optimal number of clusters (`k`) was determined using the elbow method, analyzing the Sum of Squared Errors (SSE) and the ratio of proportional change in `k` to the proportional decrease in SSE.

| k  | SSE      | Decrease in SSE | Ratio = (% Incr k) / (% Decr SSE) |
| :- | :------- | :-------------- | :-------------------------------- |
| 2  | 1294.41  | ---             | ---                               |
| 3  | 657.65   | 636.76          | 2.03                              |
| 4  | 400.67   | 256.98          | 1.28                              |
| 5  | 274.99   | 125.68          | 1.06                              |
| 6  | 182.96   | 92.03           | **0.75**                          |
| 7  | 146.14   | 36.82           | 0.99                              |
| 8  | 122.85   | 23.29           | 1.05                              |
| 9  | 90.56    | 32.29           | 0.54                              |
| 10 | 76.60    | 13.96           | 0.81                              |

Based on the analysis (lowest ratio before a consistent increase, ignoring the outlier at k=9), `optimal_k = 6` was selected and used for the final clustering. The results mapping `customer_id` to `pattern_id` (0-5) are stored in the `customer_segments` table.

### Predictive Models (RandomForestRegressor)

Two RandomForestRegressor models were trained using `age` and one-hot encoded `region` features:
**Predicting `total_transactions`**

Model evaluation metrics (MSE, R2) are logged during the `src/predictive_model.py` script execution. The trained models and preprocessors are saved to the `models/` directory.

## Gemini Code Assistant Tools Mapping

The following table illustrates the tools and capabilities the Gemini Code Assistant can leverage in relation to the technologies and concepts planned for this project:

| Your Tool/Concept                      | Gemini's Equivalent Tool/Capability                                  | How Gemini Uses It / Contributes                                                                                                |
| :------------------------------------- | :------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------ |
| **PostgreSQL / Supabase (Database)**   | `edit_file`, `python` (for `psycopg2`, `supabase-py`)                | Writes Python scripts to connect, query (extract/load data), and manage database interactions. Can help script DB CLI commands. |
| **GitHub (Version Control)**           | `terminal` (for `git` commands)                                      | Executes Git commands for repo operations (clone, add, commit, push, branch). Can help write PR descriptions.                  |
| **Context7 Docs / `mcp-server-context7`** | `resolve-library-id`, `get-library-docs`                             | Directly fetches library documentation and code snippets (e.g., for scikit-learn and its modules).                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit-learn (general) | `resolve-library-id`, `get-library-docs`                             | Fetches scikit-learn documentation and examples.                                                                                |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_bayes_inference | `resolve-library-id`, `get-library-docs`; `edit_file` (for `bayes-mcp` client) | Fetches general scikit-learn/Bayesian docs; helps write client code for your planned `bayes-mcp`.                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_random_forest_classifier | `resolve-library-id`, `get-library-docs`                             | Fetches `RandomForestClassifier` documentation from scikit-learn.                                                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_kmeans      | `resolve-library-id`, `get-library-docs`                             | Fetches `KMeans` documentation, including for elbow method.                                                                     |
| **Python (Core Language)**             | `edit_file`, `python` (via `default_api.run_code`), `diagnostics`      | Writes, understands, executes snippets, and debugs Python code for all project scripts.                                         |
| **Axiom Logging / `mcp-server-axiom`** | `edit_file`                                                          | Writes Python code (e.g., in `src/logging.py`) to use an Axiom client library for sending logs.                                 |
| **Pandas (Data Manipulation)**         | `edit_file`, `python`                                                | Writes Python code using `pandas` for data cleaning, transformation, and preparation.                                           |
| **JupyterLab (Notebooks)**             | `edit_file`                                                          | Generates Python code and Markdown content for Jupyter Notebook cells.                                                          |
| **`bayes-mcp` (Your Planned Server)**  | `edit_file`, `read_file` (to understand its API)                     | Writes Python client code (e.g., in `src/ml_model.py`) to interact with your running `bayes-mcp` server via HTTP/JSON.        |
| **`secrets-manager` (Your CLI Tool)**  | `read_file` (to understand its usage), provides CLI command          | Reads its README; constructs the `secrets-manager env-file` command for *you* to run locally to populate `.env`.              |
| **Other Planned Project MCPs** (e.g., `mcp-server-github`, `postgres-context-server` as project components) | `edit_file`, `read_file` (to understand their APIs if documented)    | Helps write client code for your Python scripts to interact with these if they are built and run as services.                |
