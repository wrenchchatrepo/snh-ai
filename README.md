# SNH-AI Project

This project implements an end-to-end data pipeline to extract customer data, transform it, and apply a machine learning model to identify customer segments.

## Overview

The pipeline consists of the following main stages:
1.  **Data _E_xtraction**: Retrieves customer data from a source.
2.  **Data _T_ransformation**: Cleans, scales, and encodes the data.
3.  **Machine Learning**: Applies a clustering model to identify patterns.
4.  **Data _L_oading**: Loads the processed data with pattern labels to a destination.

## Project Structure

```
snh-ai/
├── .env                # Environment variables (generated by setup.py, not committed)
├── .github/            # GitHub specific files (e.g., workflows)
├── .gitignore          # Specifies intentionally untracked files that Git should ignore
├── Dockerfile          # For containerizing the application
├── README.md           # This file
├── config.py           # Configuration file for the project
├── docs/               # Documentation files
│   ├── StandardScaler.md
│   └── One-hot_encode.md
├── mcp-servers/        # MCP server configurations/stubs
│   ├── bayes-mcp
│   ├── mcp-server-axiom
│   ├── mcp-server-context7
│   ├── mcp-server-github
│   ├── mcp-server-sequential-thinking
│   └── postgres-context-server
├── process.py          # Main orchestration script for the pipeline
├── requirements.txt    # Python dependencies (to be generated)
├── setup.py            # Script for project setup and initialization
├── src/                # Source code for the pipeline
│   ├── Ingest.py       # Data extraction script
│   ├── clean.py        # Data cleaning script
│   ├── encode.py       # Data encoding script
│   ├── logging.py      # Common logging module
│   ├── ml_model.py     # Machine learning model script
│   ├── presentation.py # Data loading/presentation script
│   ├── scale.py        # Data scaling script
│   └── template.json   # Template for script structure
└── start-database.sh   # Script to start/setup the database
```

## Getting Started

### Prerequisites

*   Python 3.9+
*   Access to a PostgreSQL database (Instructions assume Supabase is used, as per the setup plan)
*   Git

### Setup

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/wrenchchatrepo/snh-ai.git
    cd snh-ai
    ```

2.  **Create Python Virtual Environment:**
    (Recommended to avoid conflicting with global packages)
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate # On Windows use `.venv\Scripts\activate`
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment Variables:**
    *   Copy the example environment file: `cp .env.example .env` (or rename if `.env.example` exists - check `setup.py`)
    *   Edit the `.env` file and fill in your actual credentials for:
        *   `ANTHROPIC_API_KEY` (Optional, if used by other components)
        *   `GITHUB_PAT` (Optional, if using GitHub API features)
        *   `SUPABASE_URL`
        *   `SUPABASE_ANON` 
        *   `SUPABASE_SERVICE_ROLE` 
        *   `SUPABASE_PROJECT_ID` (Optional)
        *   `SUPABASE_JWT_SECRET` (Optional)
        *   `SUPABASE_DB_PASSWORD` (Optional, depending on connection method)
        *   `AXIOM_TOKEN`
    *   **Important:** The `AXIOM_DATASET_NAME` is currently hardcoded in `config.py` as "snh-ai". If you wish to configure it via `.env`, uncomment and set `AXIOM_DATASET_NAME=your_dataset_name` in `.env` and update the corresponding `os.getenv` call in `config.py`.
    *   **Important:** Do NOT commit the `.env` file to Git.

5.  **Set Up Database Tables:**
    *   Connect to your Supabase project (e.g., via the Supabase Dashboard SQL Editor).
    *   Execute the SQL DDL statements found in `data/data_history.md` (or use a dedicated setup script if created later - see TODO) to create the required tables:
        *   `raw_customer_data`
        *   `cleaned_customer_data`
        *   `transformed_customer_data`
        *   `customer_segments`
        *   `transaction_predictions`

### Running the Full Pipeline

To populate the database tables sequentially, run the orchestration script:

```bash
# Ensure virtual environment is active
source .venv/bin/activate

# Run the process script
python3 process.py
```
This will execute `Ingest.py`, `clean.py`, `transform.py`, and `ml_model.py` in order. Monitor the output for any errors. Logs will also be sent to Axiom if configured correctly.

### Running Individual Scripts

You can also run individual pipeline steps (after the necessary input tables exist):

```bash
python3 src/Ingest.py
python3 src/clean.py
python3 src/transform.py
python3 src/ml_model.py
python3 src/predictive_model.py # Runs predictive modeling separately
```

### Using the Analysis Notebook (`snh_analysis.ipynb`)

This notebook analyzes the customer segments generated by the pipeline.

1.  **Ensure Pipeline Has Run:** Make sure you have successfully run the full pipeline (`python3 process.py`) at least once so that the `cleaned_customer_data` and `customer_segments` tables are populated in Supabase.
2.  **Ensure Dependencies Installed:** Run `pip install -r requirements.txt` if you haven't already (includes `jupyterlab`, `matplotlib`, `seaborn`).
3.  **Start JupyterLab:**
    (From the `snh-ai` directory, with the virtual environment active)
    ```bash
    jupyter lab
    ```
4.  **Open Notebook:** JupyterLab will open in your web browser. Navigate to and open `snh_analysis.ipynb`.
5.  **Run Cells:** Execute the cells sequentially by selecting a cell and pressing `Shift + Enter` (or using the Run menu).
    *   The notebook will connect to Supabase, load the cleaned data and segment assignments, merge them, and perform analysis and visualization.
    *   Review the generated tables and plots to understand the characteristics of each customer segment.
    *   (Optional) Examine the feature importances from the saved RandomForestRegressor model.
    *   Fill in the final "Insights & Conclusion" Markdown cell with your observations.

## ML Model Details

### Customer Segmentation (KMeans)

The pipeline uses KMeans clustering to identify customer segments based on the transformed data (`age_scaled`, `annual_income_scaled`, `total_transactions`, and one-hot encoded `region`).

#### Elbow Method for Optimal k

The optimal number of clusters (`k`) was determined using the elbow method, analyzing the Sum of Squared Errors (SSE) and the ratio of proportional change in `k` to the proportional decrease in SSE.

| k  | SSE      | Decrease in SSE | Ratio = (% Incr k) / (% Decr SSE) |
| :- | :------- | :-------------- | :-------------------------------- |
| 2  | 1294.41  | ---             | ---                               |
| 3  | 657.65   | 636.76          | 2.03                              |
| 4  | 400.67   | 256.98          | 1.28                              |
| 5  | 274.99   | 125.68          | 1.06                              |
| 6  | 182.96   | 92.03           | **0.75**                          |
| 7  | 146.14   | 36.82           | 0.99                              |
| 8  | 122.85   | 23.29           | 1.05                              |
| 9  | 90.56    | 32.29           | 0.54                              |
| 10 | 76.60    | 13.96           | 0.81                              |

Based on the analysis (lowest ratio before a consistent increase, ignoring the outlier at k=9), `optimal_k = 6` was selected and used for the final clustering. The results mapping `customer_id` to `pattern_id` (0-5) are stored in the `customer_segments` table.

### Predictive Models (RandomForestRegressor)

Two RandomForestRegressor models were trained using `age` and one-hot encoded `region` features:
**Predicting `total_transactions`**

Model evaluation metrics (MSE, R2) are logged during the `src/predictive_model.py` script execution. The trained models and preprocessors are saved to the `models/` directory.

## Gemini Code Assistant Tools Mapping

The following table illustrates the tools and capabilities the Gemini Code Assistant can leverage in relation to the technologies and concepts planned for this project:

| Your Tool/Concept                      | Gemini's Equivalent Tool/Capability                                  | How Gemini Uses It / Contributes                                                                                                |
| :------------------------------------- | :------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------ |
| **PostgreSQL / Supabase (Database)**   | `edit_file`, `python` (for `psycopg2`, `supabase-py`)                | Writes Python scripts to connect, query (extract/load data), and manage database interactions. Can help script DB CLI commands. |
| **GitHub (Version Control)**           | `terminal` (for `git` commands)                                      | Executes Git commands for repo operations (clone, add, commit, push, branch). Can help write PR descriptions.                  |
| **Context7 Docs / `mcp-server-context7`** | `resolve-library-id`, `get-library-docs`                             | Directly fetches library documentation and code snippets (e.g., for scikit-learn and its modules).                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit-learn (general) | `resolve-library-id`, `get-library-docs`                             | Fetches scikit-learn documentation and examples.                                                                                |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_bayes_inference | `resolve-library-id`, `get-library-docs`; `edit_file` (for `bayes-mcp` client) | Fetches general scikit-learn/Bayesian docs; helps write client code for your planned `bayes-mcp`.                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_random_forest_classifier | `resolve-library-id`, `get-library-docs`                             | Fetches `RandomForestClassifier` documentation from scikit-learn.                                                            |
| &nbsp;&nbsp;&nbsp;&nbsp;↳ scikit_kmeans      | `resolve-library-id`, `get-library-docs`                             | Fetches `KMeans` documentation, including for elbow method.                                                                     |
| **Python (Core Language)**             | `edit_file`, `python` (via `default_api.run_code`), `diagnostics`      | Writes, understands, executes snippets, and debugs Python code for all project scripts.                                         |
| **Axiom Logging / `mcp-server-axiom`** | `edit_file`                                                          | Writes Python code (e.g., in `src/logging.py`) to use an Axiom client library for sending logs.                                 |
| **Pandas (Data Manipulation)**         | `edit_file`, `python`                                                | Writes Python code using `pandas` for data cleaning, transformation, and preparation.                                           |
| **JupyterLab (Notebooks)**             | `edit_file`                                                          | Generates Python code and Markdown content for Jupyter Notebook cells.                                                          |
| **`bayes-mcp` (Your Planned Server)**  | `edit_file`, `read_file` (to understand its API)                     | Writes Python client code (e.g., in `src/ml_model.py`) to interact with your running `bayes-mcp` server via HTTP/JSON.        |
| **`secrets-manager` (Your CLI Tool)**  | `read_file` (to understand its usage), provides CLI command          | Reads its README; constructs the `secrets-manager env-file` command for *you* to run locally to populate `.env`.              |
| **Other Planned Project MCPs** (e.g., `mcp-server-github`, `postgres-context-server` as project components) | `edit_file`, `read_file` (to understand their APIs if documented)    | Helps write client code for your Python scripts to interact with these if they are built and run as services.                |
